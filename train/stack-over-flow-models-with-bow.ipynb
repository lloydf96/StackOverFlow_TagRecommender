{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-30T03:17:23.706581Z",
     "iopub.status.busy": "2022-01-30T03:17:23.702026Z",
     "iopub.status.idle": "2022-01-30T03:17:24.589813Z",
     "shell.execute_reply": "2022-01-30T03:17:24.589254Z",
     "shell.execute_reply.started": "2022-01-29T22:25:03.609736Z"
    },
    "papermill": {
     "duration": 0.910456,
     "end_time": "2022-01-30T03:17:24.589970",
     "exception": false,
     "start_time": "2022-01-30T03:17:23.679514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "        \n",
    "import random\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:17:24.624220Z",
     "iopub.status.busy": "2022-01-30T03:17:24.623713Z",
     "iopub.status.idle": "2022-01-30T03:17:27.869522Z",
     "shell.execute_reply": "2022-01-30T03:17:27.868981Z",
     "shell.execute_reply.started": "2022-01-29T22:25:04.733207Z"
    },
    "papermill": {
     "duration": 3.264822,
     "end_time": "2022-01-30T03:17:27.869661",
     "exception": false,
     "start_time": "2022-01-30T03:17:24.604839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "qs_train = pd.read_csv('/input/qs_train.csv')\n",
    "qs_test = pd.read_csv('/input/qs_test.csv')\n",
    "tags_train = pd.read_csv('/input/tags_train.csv')\n",
    "tags_test = pd.read_csv('/input/tags_test.csv')\n",
    "\n",
    "model_tags = Word2Vec.load('/input/model_tags.model')\n",
    "model_qs = Word2Vec.load('/input/model_qs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:17:27.907534Z",
     "iopub.status.busy": "2022-01-30T03:17:27.906774Z",
     "iopub.status.idle": "2022-01-30T03:17:27.942424Z",
     "shell.execute_reply": "2022-01-30T03:17:27.941933Z",
     "shell.execute_reply.started": "2022-01-29T22:25:08.385644Z"
    },
    "papermill": {
     "duration": 0.058551,
     "end_time": "2022-01-30T03:17:27.942545",
     "exception": false,
     "start_time": "2022-01-30T03:17:27.883994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qs_train = qs_train[qs_train.word_len >= 1]\n",
    "tags_train = tags_train[tags_train.Id.isin(qs_train.Id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:17:27.983322Z",
     "iopub.status.busy": "2022-01-30T03:17:27.973986Z",
     "iopub.status.idle": "2022-01-30T03:17:27.986088Z",
     "shell.execute_reply": "2022-01-30T03:17:27.985545Z",
     "shell.execute_reply.started": "2022-01-29T22:25:08.46015Z"
    },
    "papermill": {
     "duration": 0.028989,
     "end_time": "2022-01-30T03:17:27.986223",
     "exception": false,
     "start_time": "2022-01-30T03:17:27.957234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class processText():\n",
    "    def __init__(self):\n",
    "        \n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        special_cases = {\n",
    "                        \"c#\" : [{\"ORTH\": \"csharp\"}],\n",
    "                        \"c++\" : [{\"ORTH\": \"c++\"}],\n",
    "                         \".net\" : [{\"ORTH\": \"asp.net\"}],\n",
    "                         \"asp.net\": [{\"ORTH\": \"asp.net\"}],\n",
    "                        }\n",
    "\n",
    "        nlp.tokenizer.rules = nlp.tokenizer.rules.update(special_cases)\n",
    "        suffixes = list(nlp.Defaults.suffixes)\n",
    "        suffixes.remove(\">\")\n",
    "        suffixes.remove(\":\")\n",
    "\n",
    "        suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
    "        nlp.tokenizer.suffix_search = suffix_regex.search\n",
    "\n",
    "        prefixes = list(nlp.Defaults.prefixes)\n",
    "        prefixes.remove(\"<\")\n",
    "        prefixes.remove(\":\")\n",
    "\n",
    "        prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
    "        nlp.tokenizer.prefix_search = prefix_regex.search\n",
    "        self.all_stopwords = nlp.Defaults.stop_words\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def process_sentence(self,sentence):\n",
    "        doc = self.nlp(sentence.lower())\n",
    "        return [token.text for token in doc if not self.regex_prep(token.text)]\n",
    "\n",
    "    def regex_prep(self,word):\n",
    "        #if any of them are true then do not continue\n",
    "        is_true = bool(re.search(r\"^\\W+$\",word))\n",
    "        is_true = (True if word in self.all_stopwords else False) or is_true\n",
    "        return is_true\n",
    "\n",
    "    def process(self,sentence):\n",
    "        return self.process_sentence(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:17:28.033392Z",
     "iopub.status.busy": "2022-01-30T03:17:28.022789Z",
     "iopub.status.idle": "2022-01-30T03:17:29.362306Z",
     "shell.execute_reply": "2022-01-30T03:17:29.361378Z",
     "shell.execute_reply.started": "2022-01-29T22:25:08.489096Z"
    },
    "papermill": {
     "duration": 1.362002,
     "end_time": "2022-01-30T03:17:29.362440",
     "exception": false,
     "start_time": "2022-01-30T03:17:28.000438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SOFDataset(Dataset):\n",
    "    def __init__(self,qs,tags,model_qs,model_tags,process_text,use_embedding = False,get_all_tags = False,test_on_known_tags = True,add_negative_sample = False):\n",
    "        self.qs = qs\n",
    "        self.tags = tags\n",
    "        self.model_qs = model_qs\n",
    "        self.tags.reset_index(inplace = True, drop = True)\n",
    "        self.tags['Tag'] = self.tags.Tag.apply(lambda x : self.process_tags(x))\n",
    "        self.qs_vocab = model_qs.wv.index_to_key\n",
    "        self.process_text = process_text\n",
    "        self.use_embedding = use_embedding\n",
    "        self.get_all_tags = get_all_tags\n",
    "        self.model_tags = model_tags\n",
    "        self.add_negative_sample = add_negative_sample\n",
    "        self.qs['processed_text'] = self.qs.Title.apply(lambda x:[word if word in self.qs_vocab else '<unknown_word_SOF>' for word in self.process_text.process(x)])\n",
    "        if test_on_known_tags:\n",
    "            self.tags = self.tags[self.tags.Tag.isin(model_tags.wv.index_to_key)].reset_index(drop = True)\n",
    "            self.qs = self.qs[self.qs.Id.isin(self.tags.Id)].reset_index(drop = True)\n",
    "        if not self.use_embedding :\n",
    "            self.qs_vocab_length = len(self.qs_vocab)\n",
    "            self.qs_vocab_dict = dict(zip(self.qs_vocab,list(range(self.qs_vocab_length))))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.tags.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        '''Return qs vector and tag word (for training word would be converted to vector using model_tags)'''\n",
    "        \n",
    "        tags_val = self.tags.Tag[idx]\n",
    "        tags_idx = self.tags.Id[idx]\n",
    "        tags_val = self.process_tags(tags_val)\n",
    "        qs_val = self.qs[self.qs.Id == tags_idx].processed_text.item()\n",
    "        \n",
    "        if self.use_embedding:\n",
    "            qs_val = [self.model_qs.wv[word] for word in qs_val if word in self.qs_vocab]\n",
    "            if len(qs_val) == 0:\n",
    "                qs_val = [self.model_qs.wv['<unknown_word_SOF>']]\n",
    "            qs_val = self.embedding_qs(qs_val)\n",
    "        else:\n",
    "            qs_val = self.one_hot_qs(qs_val)\n",
    "            \n",
    "        if self.get_all_tags:\n",
    "            tags_all = list(self.tags.Tag[self.tags.Id == tags_idx].values)\n",
    "            if len(tags_all)<7:\n",
    "                tags_all+=['None']*(7-len(tags_all))\n",
    "            return qs_val,tags_val,tags_all\n",
    "        elif self.add_negative_sample:\n",
    "            tags_all = list(self.tags.Tag[self.tags.Id == tags_idx].values)\n",
    "            return qs_val,tags_val, random.choice(list(set(self.model_tags.wv.index_to_key) - set(tags_all)))\n",
    "        else:\n",
    "            return qs_val,tags_val\n",
    "        \n",
    "    def one_hot_qs(self,qs_val):\n",
    "        qs_val = torch.IntTensor(np.array([self.qs_vocab_dict[word] for word in qs_val])).to(torch.int64)\n",
    "        qs_val = torch.sum(F.one_hot(qs_val,num_classes = self.qs_vocab_length),axis = 0)\n",
    "        return qs_val.to(torch.float)\n",
    "        \n",
    "    def process_tags(self,tag):\n",
    "        return str(tag).lower().strip()\n",
    "           \n",
    "    def embedding_qs(self,qs_val):\n",
    "        \n",
    "        qs_val = torch.from_numpy(np.sum(np.array(qs_val),axis = 0)).type(torch.float)\n",
    "        \n",
    "        return qs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:17:29.395743Z",
     "iopub.status.busy": "2022-01-30T03:17:29.395139Z",
     "iopub.status.idle": "2022-01-30T03:17:38.112128Z",
     "shell.execute_reply": "2022-01-30T03:17:38.111542Z",
     "shell.execute_reply.started": "2022-01-29T22:25:10.313543Z"
    },
    "papermill": {
     "duration": 8.73484,
     "end_time": "2022-01-30T03:17:38.112294",
     "exception": false,
     "start_time": "2022-01-30T03:17:29.377454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "process_text = processText()\n",
    "# train_ds = SOFDataset(qs_train,tags_train,model_qs,process_text,use_embedding = True,get_all_tags = False)\n",
    "# x_qs,x_tag = train_ds[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:17:38.157610Z",
     "iopub.status.busy": "2022-01-30T03:17:38.156656Z",
     "iopub.status.idle": "2022-01-30T03:17:38.159247Z",
     "shell.execute_reply": "2022-01-30T03:17:38.158860Z",
     "shell.execute_reply.started": "2022-01-29T22:44:12.15012Z"
    },
    "papermill": {
     "duration": 0.031326,
     "end_time": "2022-01-30T03:17:38.159351",
     "exception": false,
     "start_time": "2022-01-30T03:17:38.128025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,model_tags,metric = 'cosine',k = 5):\n",
    "        super(DNN, self).__init__()\n",
    "        self.L1 = nn.Sequential(nn.Linear(input_dim,1000),\n",
    "                                nn.BatchNorm1d(1000),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(1000,500),\n",
    "                                nn.BatchNorm1d(500),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(500,100),\n",
    "                                nn.BatchNorm1d(100),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(100,100),\n",
    "                                nn.BatchNorm1d(100),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(100,100),\n",
    "                                nn.BatchNorm1d(100),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(100,75),\n",
    "                                nn.BatchNorm1d(75),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(75,50),\n",
    "                                nn.BatchNorm1d(50),\n",
    "                                nn.ReLU()\n",
    "                                )\n",
    "\n",
    "        self.fc = nn.Linear(50,output_dim)\n",
    "        self.model_tags = model_tags\n",
    "        self.k = k\n",
    "        self.model_tags_vector = torch.FloatTensor(self.model_tags.wv.vectors)\n",
    "        self.metric = metric\n",
    "        \n",
    "    def forward(self,qs,tags):\n",
    "        tag_model_op = self.predict(qs)\n",
    "        tag_op = torch.FloatTensor([self.model_tags.wv[tag] for tag in tags])\n",
    "        return tag_model_op,tag_op\n",
    "    \n",
    "    def infer(self,qs):\n",
    "        tag_model = self.predict(qs)\n",
    "        tag_distance = [self.distance_metric(tag) for tag in tag_model]\n",
    "        nearest_k_tags = [torch.topk(i_distance,self.k).indices for i_distance in tag_distance]\n",
    "        nearest_k_tags = [ [self.model_tags.wv.index_to_key[tag_index] for tag_index in tag_list] for tag_list in nearest_k_tags]\n",
    "        return nearest_k_tags\n",
    "        \n",
    "    def predict(self,qs):\n",
    "        L1_op = self.L1(qs)\n",
    "        op = self.fc(L1_op)\n",
    "        return op\n",
    "    \n",
    "    def distance_metric(self,tag):\n",
    "        \n",
    "        if self.metric == 'cosine':\n",
    "#             print((tag @ self.model_tags_vector.T).shape)\n",
    "#             print(((tag.data.norm(2))* self.model_tags_vector.T.norm(2,dim = 0)).shape)\n",
    "            return (tag @ self.model_tags_vector.T) / ((tag.data.norm(2))* self.model_tags_vector.T.norm(2,dim = 0))\n",
    "        else :\n",
    "#             print(torch.sum((tag - self.model_tags_vector)**2,dim = 1).shape)\n",
    "            return -1*torch.sum((tag - self.model_tags_vector)**2,dim = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:17:38.206231Z",
     "iopub.status.busy": "2022-01-30T03:17:38.189554Z",
     "iopub.status.idle": "2022-01-30T03:17:38.208648Z",
     "shell.execute_reply": "2022-01-30T03:17:38.208233Z",
     "shell.execute_reply.started": "2022-01-29T22:25:20.790388Z"
    },
    "papermill": {
     "duration": 0.035594,
     "end_time": "2022-01-30T03:17:38.208762",
     "exception": false,
     "start_time": "2022-01-30T03:17:38.173168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "def log_statement(statement = None):\n",
    "    if statement is None:\n",
    "        with open('logs.txt','w') as f:\n",
    "            f.write('Logging started... ')\n",
    "            \n",
    "    else:\n",
    "        with open('logs.txt','a') as f:\n",
    "            f.write('\\n'+statement)\n",
    "            \n",
    "def eval_func(model,dl,max_steps = 50,training = True):\n",
    "    dl.dataset.get_all_tags = True\n",
    "    tag_exists = 0\n",
    "    samples = 0\n",
    "    loss = 0\n",
    "    for step,(qs,tag,all_tags) in enumerate(dl):\n",
    "        qs,tag,all_tags = qs,tag,all_tags\n",
    "        if training:\n",
    "            model_op = model(qs,tag)\n",
    "            loss+= loss_function(model_op[0],model_op[1],target = torch.tensor([1]))\n",
    "        recommended_tags = model.infer(qs)\n",
    "        all_tags = np.array(all_tags).T\n",
    "        tag_exists += sum([1 for i,tags in enumerate(recommended_tags) if len(set(all_tags[i]).intersection(set(tags))) > 0])\n",
    "        samples += qs.shape[0]\n",
    "        if step > max_steps:\n",
    "            break\n",
    "        \n",
    "    dl.dataset.get_all_tags = False\n",
    "    \n",
    "    return loss/samples,tag_exists/samples\n",
    "        \n",
    "def train(model,optimizer,scheduler,loss_function,train_dl,test_dl,epochs,max_samples_per_epoch,device):\n",
    "    model.to(device)\n",
    "    loss_function.to(device)\n",
    "    log_statement()\n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch No : \",i)\n",
    "        model.train()\n",
    "        for step,(qs,tag,neg_tag) in enumerate(train_dl):\n",
    "            qs,tag = qs.to(device),tag\n",
    "            model.zero_grad()\n",
    "            model_op = model(qs,tag)\n",
    "            neg_tag = torch.FloatTensor(model.model_tags.wv[neg_tag]).to(device)\n",
    "            predicted = model_op[0].to(device)\n",
    "            target = model_op[1].to(device)\n",
    "            positive_case = torch.tensor([1]).to(device)\n",
    "            negative_case = torch.tensor([-1]).to(device)\n",
    "            loss = loss_function(predicted,target,target = positive_case) + loss_function(predicted,neg_tag,target = negative_case)\n",
    "             \n",
    "            #model(qs) will give both ip and op torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm = 2,norm_type = 2)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm = 1,norm_type = 'inf')\n",
    "            optimizer.step()\n",
    "            \n",
    "#             print(\"\\nmodel weights\")\n",
    "#             print(model.L1[0].weight)\n",
    "#             print(\"\\nmodel grad\")\n",
    "#             print(model.L1[0].weight.grad)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                print('\\nloss')\n",
    "                print(loss.detach())\n",
    "                print('model_grad')\n",
    "                print(model.L1[0].weight.grad.data.norm(2))\n",
    "                if step>max_samples_per_epoch//batch_size:\n",
    "                    break\n",
    "        \n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print(\"\\nmodel weights\")\n",
    "            print(model.L1[0].weight)\n",
    "            print(\"\\nmodel grad\")\n",
    "            print(model.L1[0].weight.grad)\n",
    "            model.to('cpu')\n",
    "            loss_function.to('cpu')\n",
    "            train_loss,train_model_metric = eval_func(model,train_dl,max_samples_per_epoch//batch_size)\n",
    "            test_loss,test_model_metric = eval_func(model,test_dl,training = False)\n",
    "            \n",
    "            statement = \"Train loss is %f and Train model metric is %f\"%(train_loss,train_model_metric)\n",
    "            print(statement)\n",
    "            log_statement(statement)\n",
    "            \n",
    "            statement = \"Test loss is %f and Test model metric is %f\"%(test_loss,test_model_metric)\n",
    "            print(statement)\n",
    "            log_statement(statement)\n",
    "            model.to(device)\n",
    "            loss_function.to(device)\n",
    "            \n",
    "    return model\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:17:38.252230Z",
     "iopub.status.busy": "2022-01-30T03:17:38.251516Z",
     "iopub.status.idle": "2022-01-30T03:19:15.672149Z",
     "shell.execute_reply": "2022-01-30T03:19:15.673251Z",
     "shell.execute_reply.started": "2022-01-29T22:25:20.828715Z"
    },
    "papermill": {
     "duration": 97.450491,
     "end_time": "2022-01-30T03:19:15.673477",
     "exception": false,
     "start_time": "2022-01-30T03:17:38.222986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 2048\n",
    "test_ds = SOFDataset(qs_test,tags_test,model_qs,model_tags,process_text,use_embedding = False,get_all_tags = False,test_on_known_tags = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:19:15.728379Z",
     "iopub.status.busy": "2022-01-30T03:19:15.727485Z",
     "iopub.status.idle": "2022-01-30T03:19:15.776599Z",
     "shell.execute_reply": "2022-01-30T03:19:15.777038Z",
     "shell.execute_reply.started": "2022-01-29T22:43:15.169325Z"
    },
    "papermill": {
     "duration": 0.079096,
     "end_time": "2022-01-30T03:19:15.777168",
     "exception": false,
     "start_time": "2022-01-30T03:19:15.698072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4081"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[1][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:19:15.811817Z",
     "iopub.status.busy": "2022-01-30T03:19:15.810971Z",
     "iopub.status.idle": "2022-01-30T03:27:11.345127Z",
     "shell.execute_reply": "2022-01-30T03:27:11.344512Z",
     "shell.execute_reply.started": "2022-01-29T22:32:03.680209Z"
    },
    "papermill": {
     "duration": 475.553854,
     "end_time": "2022-01-30T03:27:11.345295",
     "exception": false,
     "start_time": "2022-01-30T03:19:15.791441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "train_ds = SOFDataset(qs_train,tags_train,model_qs,model_tags,process_text,use_embedding = False,get_all_tags = False,add_negative_sample = True)\n",
    "test_ds = SOFDataset(qs_test,tags_test,model_qs,model_tags,process_text,use_embedding = False,get_all_tags = False,test_on_known_tags = True)\n",
    "train_dl = DataLoader(train_ds, batch_size= batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:27:11.382058Z",
     "iopub.status.busy": "2022-01-30T03:27:11.381499Z",
     "iopub.status.idle": "2022-01-30T06:22:01.903072Z",
     "shell.execute_reply": "2022-01-30T06:22:01.903534Z",
     "shell.execute_reply.started": "2022-01-29T22:44:16.751291Z"
    },
    "papermill": {
     "duration": 10490.543558,
     "end_time": "2022-01-30T06:22:01.903731",
     "exception": false,
     "start_time": "2022-01-30T03:27:11.360173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CosineEmbeddingLoss()\n",
    "epochs = 30\n",
    "max_samples_per_epoch = 20000\n",
    "input_dim = test_ds[0][0].shape[0]\n",
    "model = DNN(input_dim,10,model_tags)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dl.dataset.get_all_tags = False\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs,eta_min = 5e-7)\n",
    "model = train(model,optimizer,scheduler,loss_function,train_dl,test_dl,epochs,max_samples_per_epoch*4,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:02.842963Z",
     "iopub.status.busy": "2022-01-30T06:22:02.842404Z",
     "iopub.status.idle": "2022-01-30T06:22:02.881794Z",
     "shell.execute_reply": "2022-01-30T06:22:02.881245Z"
    },
    "papermill": {
     "duration": 0.546192,
     "end_time": "2022-01-30T06:22:02.881913",
     "exception": false,
     "start_time": "2022-01-30T06:22:02.335721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.save(model.state_dict(), '/model/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:03.622380Z",
     "iopub.status.busy": "2022-01-30T06:22:03.621485Z",
     "iopub.status.idle": "2022-01-30T06:22:03.623314Z",
     "shell.execute_reply": "2022-01-30T06:22:03.623723Z",
     "shell.execute_reply.started": "2022-01-28T07:10:04.280197Z"
    },
    "papermill": {
     "duration": 0.387936,
     "end_time": "2022-01-30T06:22:03.623863",
     "exception": false,
     "start_time": "2022-01-30T06:22:03.235927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter=collections.Counter(tags_train.prepped_tags)\n",
    "total_words = np.max(np.array(list(counter.values())))\n",
    "recommended_tags = [word for word,val in sorted(counter.items(),key=lambda x : -1*x[1])[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:04.322923Z",
     "iopub.status.busy": "2022-01-30T06:22:04.322120Z",
     "iopub.status.idle": "2022-01-30T06:22:04.324557Z",
     "shell.execute_reply": "2022-01-30T06:22:04.324139Z",
     "shell.execute_reply.started": "2022-01-28T07:25:15.055949Z"
    },
    "papermill": {
     "duration": 0.359184,
     "end_time": "2022-01-30T06:22:04.324694",
     "exception": false,
     "start_time": "2022-01-30T06:22:03.965510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_baseline(recommended_tags,dl,max_steps = 50):\n",
    "    samples = 0\n",
    "    tag_exists = 0\n",
    "    no_of_tags = 0\n",
    "    dl.dataset.get_all_tags = True\n",
    "    for step,(qs,tag,all_tags) in enumerate(dl):\n",
    "        qs,tag,all_tags = qs,tag,all_tags\n",
    "\n",
    "        all_tags = np.array(all_tags).T\n",
    "        tag_exists +=  sum([1 for tags in all_tags if len(set(tags).intersection(set(recommended_tags))) > 0])\n",
    "        no_of_tags += sum([ len(set(tags).intersection(set(recommended_tags))) for tags in all_tags])\n",
    "        samples += qs.shape[0]\n",
    "        if step > max_steps:\n",
    "            break\n",
    "    \n",
    "    dl.dataset.get_all_tags = True\n",
    "        \n",
    "    return tag_exists/samples,no_of_tags/samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:05.015345Z",
     "iopub.status.busy": "2022-01-30T06:22:05.014483Z",
     "iopub.status.idle": "2022-01-30T06:22:05.016829Z",
     "shell.execute_reply": "2022-01-30T06:22:05.016338Z",
     "shell.execute_reply.started": "2022-01-28T07:11:22.515888Z"
    },
    "papermill": {
     "duration": 0.34895,
     "end_time": "2022-01-30T06:22:05.016955",
     "exception": false,
     "start_time": "2022-01-30T06:22:04.668005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# batch_size = 2048\n",
    "# test_ds = SOFDataset(qs_test,tags_test,model_qs,model_tags,process_text,use_embedding = True,get_all_tags = False,test_on_known_tags = True)\n",
    "# test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "# = DNN(100,10,model_tags)\n",
    "# model.load_state_dict(torch.load('../input/sof-model/model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:05.711110Z",
     "iopub.status.busy": "2022-01-30T06:22:05.710232Z",
     "iopub.status.idle": "2022-01-30T06:22:05.712129Z",
     "shell.execute_reply": "2022-01-30T06:22:05.712500Z",
     "shell.execute_reply.started": "2022-01-28T07:26:23.642752Z"
    },
    "papermill": {
     "duration": 0.353931,
     "end_time": "2022-01-30T06:22:05.712656",
     "exception": false,
     "start_time": "2022-01-30T06:22:05.358725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_eval(model,dl,max_steps = 50,training = True):\n",
    "    dl.dataset.get_all_tags = True\n",
    "    tag_exists = 0\n",
    "    samples = 0\n",
    "    loss = 0\n",
    "    no_of_tags = 0\n",
    "    \n",
    "    for step,(qs,tag,all_tags) in enumerate(dl):\n",
    "        qs,tag,all_tags = qs.to('cpu'),tag,all_tags\n",
    "        recommended_tags = model.infer(qs)\n",
    "        all_tags = np.array(all_tags).T\n",
    "        tag_exists += sum([1 for i,tags in enumerate(recommended_tags) if len(set(all_tags[i]).intersection(set(tags))) > 0])\n",
    "        no_of_tags += sum([len(set(all_tags[i]).intersection(set(tags))) for i,tags in enumerate(recommended_tags)])\n",
    "        samples += qs.shape[0]\n",
    "        if step > max_steps:\n",
    "            break\n",
    "        \n",
    "    dl.dataset.get_all_tags = False\n",
    "    \n",
    "    return tag_exists/samples,no_of_tags/samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T06:22:06.404922Z",
     "iopub.status.busy": "2022-01-30T06:22:06.403848Z",
     "iopub.status.idle": "2022-01-30T06:34:47.385556Z",
     "shell.execute_reply": "2022-01-30T06:34:47.386009Z",
     "shell.execute_reply.started": "2022-01-28T07:26:24.340732Z"
    },
    "papermill": {
     "duration": 761.330593,
     "end_time": "2022-01-30T06:34:47.386162",
     "exception": false,
     "start_time": "2022-01-30T06:22:06.055569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top 5 accuracy are : \n",
      "Model percent questions with one correct tag: 0.590498 \n",
      "number of relevant predictions on average: 0.807004\n",
      "\n",
      "Baseline percent questions with one correct tag: 0.422500 \n",
      "number of relevant predictions on average: 0.440958\n",
      "\n",
      "\n",
      "Top 7 accuracy are : \n",
      "Model percent questions with one correct tag: 0.634382 \n",
      "number of relevant predictions on average: 0.916592\n",
      "\n",
      "Baseline percent questions with one correct tag: 0.525711 \n",
      "number of relevant predictions on average: 0.551361\n",
      "\n",
      "\n",
      "Top 10 accuracy are : \n",
      "Model percent questions with one correct tag: 0.676188 \n",
      "number of relevant predictions on average: 1.038954\n",
      "\n",
      "Baseline percent questions with one correct tag: 0.597608 \n",
      "number of relevant predictions on average: 0.684948\n",
      "\n",
      "\n",
      "Top 15 accuracy are : \n",
      "Model percent questions with one correct tag: 0.719705 \n",
      "number of relevant predictions on average: 1.190918\n",
      "\n",
      "Baseline percent questions with one correct tag: 0.663556 \n",
      "number of relevant predictions on average: 0.834040\n",
      "\n",
      "\n",
      "Top 20 accuracy are : \n",
      "Model percent questions with one correct tag: 0.749694 \n",
      "number of relevant predictions on average: 1.303113\n",
      "\n",
      "Baseline percent questions with one correct tag: 0.727895 \n",
      "number of relevant predictions on average: 0.942303\n",
      "\n",
      "\n",
      "Top 30 accuracy are : \n",
      "Model percent questions with one correct tag: 0.784533 \n",
      "number of relevant predictions on average: 1.460476\n",
      "\n",
      "Baseline percent questions with one correct tag: 0.797225 \n",
      "number of relevant predictions on average: 1.078376\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "for k in [5,7,10,15,20,30]:\n",
    "    model.k = k\n",
    "    recommended_tags = [word for word,val in sorted(counter.items(),key=lambda x : -1*x[1])[:k]]\n",
    "    questions_predicted,tags_predicted = model_eval(model,test_dl)\n",
    "    baseline_q_predicted,baseline_tags_predicted = eval_baseline(recommended_tags,test_dl,1000)\n",
    "    print(\"\\n\\nTop %d accuracy are : \"%(k))\n",
    "    print(\"Model percent questions with one correct tag: %f \\nnumber of relevant predictions on average: %f\"%(questions_predicted,tags_predicted))\n",
    "    print(\"\\nBaseline percent questions with one correct tag: %f \\nnumber of relevant predictions on average: %f\"%(baseline_q_predicted,baseline_tags_predicted))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.339348,
     "end_time": "2022-01-30T06:34:48.065872",
     "exception": false,
     "start_time": "2022-01-30T06:34:47.726524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11856.149001,
   "end_time": "2022-01-30T06:34:51.641148",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-30T03:17:15.492147",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
